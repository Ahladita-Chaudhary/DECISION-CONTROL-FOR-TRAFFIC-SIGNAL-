\documentclass[11pt]{article}

% ===================== PACKAGES =====================
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{xcolor}

% ===================== HEADER & FOOTER =====================
\pagestyle{fancy}
\fancyhf{}
\lhead{CSE 400: Fundamentals of Probability in Computing}
\rhead{Lecture 5 Scribe}
\cfoot{\thepage}

% ===================== TITLE =====================
\title{
\normalsize School of Engineering and Applied Science (SEAS), Ahmedabad University\\
\vspace{0.2cm}
\textbf{CSE 400: Fundamentals of Probability in Computing}\\
\Large Lecture 5: Bayes’ Theorem, Random Variables, and PMF
}
\author{}
\date{}

\begin{document}
\maketitle
\vspace{-2cm}

\begin{center}
\begin{tabular}{ll}
\textbf{Instructor:} & {Dhaval Patel, PhD} \\[1ex]
\textbf{Date:} & {January 20\textsuperscript{th}, 2026}
\end{tabular}
\end{center}

\hrule
\vspace{0.5cm}

% ===================== LECTURE CONTENT =====================
\begin{enumerate}[leftmargin=*]

% -------------------------------------------------
\item \textbf{Bayes’ Theorem: Weighted Average of Conditional Probabilities}

Let $A$ and $B$ be events. We may express $A$ as
\[
A = AB \cup AB^{c}
\]
for, in order for an outcome to be in $A$, it must either be in both $A$ and $B$ or be in $A$ but not in $B$.

As $AB$ and $AB^{c}$ are mutually exclusive, by Axiom 3,
\[
\Pr(A) = \Pr(AB) + \Pr(AB^{c})
\]
\[
= \Pr(A \mid B)\Pr(B) + \Pr(A \mid B^{c})[1 - \Pr(B)]
\]

The probability of event $A$ is a weighted average of the conditional probabilities with weights given as the probability of the event on which it is conditioned has of occurring.

% -------------------------------------------------
\item \textbf{Bayes’ Theorem: Learning by Example}

\textbf{Example 3.1 (Part 1)}

An insurance company believes that people can be divided into two classes: those who are accident prone and those who are not. The company’s statistics show that an accident-prone person will have an accident within a fixed 1-year period with probability $0.4$, whereas this probability decreases to $0.2$ for a person who is not accident prone. If $30\%$ of the population is accident prone, find the probability that a new policyholder will have an accident within a year.

\textbf{Solution:}

Let $A_1$ denote the event that the policyholder has an accident within a year, and let $A$ denote the event that the policyholder is accident prone.

\[
\Pr(A_1) = \Pr(A_1 \mid A)\Pr(A) + \Pr(A_1 \mid A^c)\Pr(A^c)
\]
\[
= (0.4)(0.3) + (0.2)(0.7) = 0.26
\]

% -------------------------------------------------
\item \textbf{Bayes Formula: Learning by Example}

\textbf{Example 3.1 (Part 2)}

Suppose that a new policyholder has an accident within a year of purchasing a policy. What is the probability that the policyholder is accident prone?

\textbf{Solution:}

\[
\Pr(A \mid A_1) = \frac{\Pr(A \cap A_1)}{\Pr(A_1)}
\]
\[
= \frac{\Pr(A)\Pr(A_1 \mid A)}{\Pr(A_1)}
\]
\[
= \frac{(0.3)(0.4)}{0.26} = \frac{6}{13}
\]

% -------------------------------------------------
\item \textbf{Law of Total Probability}

If $B_1, B_2, \dots, B_n$ are mutually exclusive and exhaustive events, then
\[
\Pr(A) = \sum_{i=1}^{n} \Pr(A \cap B_i)
\]

This is known as the \textbf{Law of Total Probability} (Formula 3.4).

% -------------------------------------------------
\item \textbf{Bayes Formula (Proposition 3.1)}

Using
\[
\Pr(A \cap B_i) = \Pr(B_i \mid A)\Pr(A)
\]

we obtain the Bayes Formula:
\[
\Pr(B_i \mid A) = \frac{\Pr(A \mid B_i)\Pr(B_i)}{\sum_{j} \Pr(A \mid B_j)\Pr(B_j)}
\]

Here,
\begin{itemize}
\item $\Pr(B_i)$ is the \textit{a priori probability}
\item $\Pr(B_i \mid A)$ is the \textit{a posteriori probability}
\end{itemize}

% -------------------------------------------------
\item \textbf{Bayes Formula: Card Example}

Three cards are mixed: one red-red, one black-black, and one red-black. A card is selected randomly and placed face up. Given that the upper side is red, find the probability that the other side is black.

Let $RR$, $BB$, and $RB$ denote the three cards. Let $R$ denote the event that the upper side is red.

\[
\Pr(RB \mid R) =
\frac{\Pr(R \mid RB)\Pr(RB)}
{\Pr(R \mid RR)\Pr(RR) + \Pr(R \mid RB)\Pr(RB) + \Pr(R \mid BB)\Pr(BB)}
\]

\[
= \frac{(1/2)(1/3)}{(1)(1/3) + (1/2)(1/3) + (0)(1/3)} = \frac{1}{3}
\]

% -------------------------------------------------
\item \textbf{Random Variables: Motivation and Concept}

A random variable is a real-valued function defined on the sample space.

Values are determined by the outcomes of an experiment.

Probabilities are assigned to possible values of random variables.

% -------------------------------------------------
\item \textbf{Random Variable Example}

Suppose an experiment consists of tossing 3 fair coins. Let $Y$ denote the number of heads.

\[
\Pr(Y=0)=\frac{1}{8}, \quad
\Pr(Y=1)=\frac{3}{8}, \quad
\Pr(Y=2)=\frac{3}{8}, \quad
\Pr(Y=3)=\frac{1}{8}
\]

Since $Y$ must take one of the values $0,1,2,3$,
\[
\sum_y \Pr(Y=y)=1
\]

% -------------------------------------------------
\item \textbf{Probability Mass Function}

A random variable that can take on at most a countable number of possible values is said to be \textbf{discrete}.

Let $X$ be a discrete random variable with range
\[
R_X = \{x_1, x_2, x_3, \dots\}
\]

The function
\[
p(x_k) = \Pr(X = x_k)
\]
is called the \textbf{Probability Mass Function (PMF)} of $X$.

Since $X$ must take one of the values $x_k$,
\[
\sum_k p(x_k) = 1
\]

\end{enumerate}

\bigskip
\hrule
\vspace{0.2cm}

\begin{center}
\small \textit{End of Lecture 5 Scribe}
\end{center}

\end{document}
